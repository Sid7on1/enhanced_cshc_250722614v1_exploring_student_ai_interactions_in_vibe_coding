{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.HC_2507.22614v1_Exploring_Student_AI_Interactions_in_Vibe_Coding",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.HC_2507.22614v1_Exploring-Student-AI-Interactions-in-Vibe-Coding with content analysis. Detected project type: agent (confidence score: 6 matches).",
    "key_algorithms": [
      "Enhanced",
      "Code",
      "Aloud",
      "Students",
      "Paced",
      "During",
      "Flask",
      "Their",
      "Might",
      "Student"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.HC_2507.22614v1_Exploring-Student-AI-Interactions-in-Vibe-Coding.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nExploring Student-AI Interactions in Vibe Coding\nFrancis Geng\nUniversity of California San Diego\nLa Jolla, California, USA\nfgeng@ucsd.eduAnshul Shah\nUniversity of California San Diego\nLa Jolla, California, USA\nayshah@ucsd.eduHaolin Li\nUniversity of California San Diego\nLa Jolla, California, USA\nhal180@ucsd.edu\nNawab Mulla\nUniversity of California San Diego\nLa Jolla, California, USA\nnmulla@ucsd.eduSteven Swanson\nUniversity of California San Diego\nLa Jolla, California, USA\nsjswanson@ucsd.eduGerald Soosai Raj\nUniversity of California San Diego\nLa Jolla, California, USA\nasoosairaj@ucsd.edu\nDaniel Zingaro\nUniversity of Toronto Mississauga\nToronto, Ontario, Canada\ndaniel.zingaro@utoronto.caLeo Porter\nUniversity of California San Diego\nLa Jolla, California, USA\nleporter@ucsd.edu\nAbstract\nBackground and Context. Chat-based and inline-coding-based\nGenAI has already had substantial impact on the CS Education\ncommunity. The recent introduction of \u201cvibe coding\u201d may further\ntransform how students program, as it introduces a new way for\nstudents to create software projects with minimal oversight.\nObjectives. The purpose of this study is to understand how stu-\ndents in introductory programming and advanced software engi-\nneering classes interact with a vibe coding platform (Replit) when\ncreating software and how the interactions differ by programming\nbackground.\nMethods. Interview participants were asked to think-aloud while\nbuilding a web application using Replit. Thematic analysis was\nthen used to analyze the video recordings with an emphasis on the\ninteractions between the student and Replit.\nFindings. For both groups, the majority of student interactions\nwith Replit were to test or debug the prototype and only rarely\ndid students visit code. Prompts by advanced software engineering\nstudents were much more likely to include relevant app feature\nand codebase contexts than those by introductory programming\nstudents.\nCCS Concepts\n\u2022Social and professional topics \u2192Computing education .\nKeywords\nLarge Language Models, Vibe Coding, Novice Programmers, Ob-\nservation Study\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nConference 2025, Location\n\u00a92025 Copyright held by the owner/author(s).\nACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn1 Introduction\nThe advent of GenAI has had considerable impact on comput-\ning education with GenAI being capable of solving course assign-\nments [ 5,7], offering skilled AI tutors to students [ 6,16,20,23],\naltering how we teach [ 15,36], and changing the skills we need\nto teach [ 17,26,37]. LLMs, such as Copilot and ChatGPT, have\nchanged the workflow of programming for professionals [ 2] as\nwell as for students [ 26,37]. As the CS education research com-\nmunity works to identify the impact of GenAI on student learning\n(with mixed findings [ 13,21,29,36]), platforms such as Replit [ 30],\nCopilot Agents [ 22], and Cursor [ 4] have the potential to alter the\nlandscape even further.\nThe term \u201cvibe coding\u201d [ 12] was introduced this past year to\ndescribe the process of creating software with minimal effort, specif-\nically where the user does not systematically review, test, or un-\nderstand all the code produced and is focused on the end prod-\nuct [ 12,31,32]. Existing studies of vibe coding have primarily ex-\namined how experienced developers and professional programmers\nengage in this workflow, leaving a notable gap in our understand-\ning of how computing students might approach it. This gap raises\nnew questions for the CS education community, including how pro-\ngramming students of various levels might approach vibe coding.\nIf we can understand how students use AI-powered tools for vibe\ncoding, we will be better equipped to help support students learn\nto use them effectively. To study these behaviors, we examine how\nprogramming students interact with a vibe coding platform in an\nobservational experiment. As vibe coding could potentially lower\nthe barrier to building software projects, our study focuses on how\nboth introductory andadvanced CS students approach vibe coding.\nOur observational study recruited students from an introductory\nprogramming class and an advanced software engineering class.\nEach student then came to a study session where they were asked to\ncreate a web application by vibe coding in Replit. We qualitatively\nanalyzed the recordings to label the types of interactions students\nhave with Replit. We report on the behaviors of students overall and\nalso compare the behaviors of students from an introductory class\nwith those from an advanced class, focusing on their interactions\nwith the AI tool.arXiv:2507.22614v1  [cs.HC]  30 Jul 2025\n\n--- Page 2 ---\nConference 2025, 2025, Location Geng et al.\nWe find that students, in general, interact with the code produced\nby Replit very rarely and the majority of interactions with Replit\nare to prompt it for changes (primarily debugging) and interactions\nwith the prototype (mostly testing). This shift in focus to testing\nand debugging emphasizes the importance of teaching these skills\nin CS curricula. When comparing the introductory and advanced\nstudents, we find that advanced students are more willing to work\nwith the code produced by the AI (although this is still a very small\nfraction of their interactions with the tool) and that introductory\nstudent prompts are less apt to include the relevant context. These\nfindings suggest that advanced students are interacting with such\ntools in a more sophisticated manner, supporting the need to teach\nstudents of all levels how to read code and express their AI prompts\nwith programming context.\n2 Background and Literature Review\n2.1 Research on Vibe Coding\nIn this section, we explore how vibe coding has been defined, sum-\nmarize the existing research on vibe coding, and compare vibe\ncoding to agentic coding as well as first-generation coding with\ngenerative AI.\n2.1.1 What is Vibe Coding? The term \u201cvibe coding\u201d was introduced\nin February 2025 by Andrej Karpathy, a computer scientist and AI\nresearcher who co-founded OpenAI. The following tweet text is\nthe definition of vibe coding as written by Karpathy [12]:\n\u201cThere\u2019s a new kind of coding I call \u2018vibe coding\u2019, where you fully\ngive in to the vibes, embrace exponentials, and forget that the code even\nexists. It\u2019s possible because the LLMs (e.g., Cursor Composer w Sonnet)\nare getting too good. Also I just talk to Composer with SuperWhisper\nso I barely even touch the keyboard. I ask for the dumbest things like\n\u2018decrease the padding on the sidebar by half\u2019 because I\u2019m too lazy to\nfind it. I \u2018Accept All\u2019 always, I don\u2019t read the diffs anymore. When I\nget error messages I just copy paste them in with no comment, usually\nthat fixes it. The code grows beyond my usual comprehension, I\u2019d have\nto really read through it for a while. Sometimes the LLMs can\u2019t fix a\nbug so I just work around it or ask for random changes until it goes\naway. It\u2019s not too bad for throwaway weekend projects, but still quite\namusing. I\u2019m building a project or webapp, but it\u2019s not really coding \u2014\nI just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly\nworks. \"\nThat is, vibe coding involves asking a GenAI tool for code, but not\nreading that code. To debug, one can paste error messages into the\nAI, or ask for random changes until it\u2019s fixed. Karpathy notes that\nthis approach is good enough for weekend projects.\nTo be clear, Karpathy (an expert computer scientist and AI re-\nsearcher) doesn\u2019t need to vibe code. But, because it works for him,\nhe uses it. It\u2019s okay if vibe coding only \u201cusually\u201d fixes the bug, or\nonly \u201cmostly\u201d works, because Karpathy can just fix the code in\nthese cases. That is, it\u2019s perhaps unsurprising that experts, with\ndeep knowledge of the underlying code, can ride the vibes until\nsomething goes wrong, fix the problem, and continue vibing. This\nleaves us with two questions: what does vibe coding actually look\nlike when carried out by experts? And, what happens when novices\nand intermediate programmers vibe code? There is initial research\ninto the first question that we describe next; and answering the\nsecond question is a goal of the present study.2.1.2 What Do Expert Developers Do When They Vibe Code? The\nmost comprehensive study of developers vibe coding is the study\nby Sarkar and Drosos [ 32]. These authors studied YouTube and\nTwitch videos of experienced developers who were vibe coding\nwhile thinking aloud. The authors chose to analyze those videos\nwhere the programmer self-described what they are doing as vibe\ncoding, rather than attempting to impose some criteria of what\nvibe coding is. That\u2019s because the definition of vibe coding is not\nup to us, but is and will continue to be negotiated by communities\nof developers who use it in practice.\nThe authors found that these programmers benefited from their\nexpertise in several ways, such as when evaluating, testing, or\nmanually editing code, and when including detailed technical spec-\nifications in their prompts. They are able to rapidly assess code\nand make immediate judgments about its suitability. The authors\nuse the term \u201cmaterial disengagement\u201d to emphasize the increas-\ning distance between developers and their code in a vibe coding\nworkflow. That is, these developers work on their code not directly,\nbut mediated through GenAI.\nThe authors\u2019 qualitative analysis led to nine top-level categories,\nincluding what developers want to build with vibe coding, plans for\nhow they\u2019ll build it, the vibe coding workflow, prompting strategies,\nand debugging. As a sample of what the authors found, we highlight\njust three of their many insights: 1) Some developers begin with\nexpectations of complete success, but often need to temper those\nexpectations to partial (e.g., 80%) success; 2) Due to its conversa-\ntional nature, vibe coding often helped these developers go beyond\ntheir initial visions. At the same time, vibe coding could be so fast\nas to lock a developer into a suboptimal plan before they know it; 3)\nDevelopers used both single-objective prompts and multi-objective\nprompts, the latter of which may include directives to the AI about\nmultiple unrelated requirements.\nAgain, all of these findings are from expert programmers with\nconsiderable understanding of programming languages, or the af-\nfordances of AI models, or both. In contrast, the novice and inter-\nmediate programmers in our study have substantially less experi-\nence in software development and AI, making our study a valuable\ncomplement to prior work and helping to fill a critical gap in our\nunderstanding of vibe coding among non-experts and students.\n2.1.3 Vibe Coding vs. Other AI Programming Workflows. While\ndefinitions of AI-assisted programming workflows are in flux, we\ndo wish to contrast vibe coding against two other workflows in\norder to further position vibe coding.\nFirst, we distinguish vibe coding from first-generation GenAI\nprogramming workflows from 2022 and 2023, where programmers\nwould prompt for each function and the AI completed the code [ 32].\n(Chat interfaces hadn\u2019t been integrated into the GenAI tools yet!)\nVibe coding, as we have described, is much further abstracted from\nthe code, allowing programmers to delegate significantly larger\ntasks to the AI. That said, it is notentirely hands off [32].\nSecond, we distinguish vibe coding from \u201cagentic coding\u201d [ 31],\nin which the intention isto be hands off. The human is not in\nthe loop: \u201cagentic coding enables autonomous software development\nthrough goal-driven agents capable of planning, executing, testing,\nand iterating tasks with minimal human intervention\u201d [31]. At least,\nthis is how agentic coding is defined for experienced developers. We\n\n--- Page 3 ---\nExploring Student-AI Interactions in Vibe Coding Conference 2025, 2025, Location\nwonder to what extent our comparably less experienced students\u2019\n\u201cvibe coding\u201d looks like agentic coding.\n2.2 How Programmers Interact with AI\nProgramming Tools\nThough research related specifically to \u201cvibe coding\u201d is in its early\nstages, a larger body of work has studied how programmers interact\nwith generative AI programming tools. In this section, we will de-\nscribe the literature related to how experienced and inexperienced\nprogrammers interact with AI tools such as GitHub Copilot and\nChatGPT.\n2.2.1 How Experienced Programmers Use AI Tools. Broadly, the\nsoftware engineering literature on AI programming tools discussed\nhow these tools can change the software development process [ 3,\n11,24,40], how developers use AI tools [ 2,8,34,35], and challenges\nthey face when using those tools [ 19,25,38,39]. In this section, we\nwill focus on how experienced programmers use AI tools. This helps\ninform how advanced software engineering students (intermediate-\nlevel programmers) might interact with AI in a vibe coding platform.\nBarke et al .conducted an observational study of 20 programmers\nwith a range of prior experience \u2014 from professional to occasional\nprogrammers \u2014 to theorize how programmers use GitHub Copilot\n[2]. Their theory posits that programmers interact with Copilot in\ntwo modes: exploration mode, in which the programmer uses Copi-\nlot to understand how to get started with a task; and acceleration\nmode, in which programmers are aware of the necessary steps and\nuse Copilot to speed up implementation [ 2]. Recent observational\nstudies of how programmers use AI tools support the theory above,\nshedding light on the prompting and verification behaviors among\ndevelopers [ 18,28,38]. Notably, Liang et al .found that program-\nmers used various techniques to verify AI-generated code output,\nsuch as by scanning the code for keywords, using a compiler to\ndetect issues, executing the code, or examining the code in depth\n[18]. In addition, Vaithilingam et al .and Perry et al .both discussed\nchallenges developers face, with Vaithilingam et al .highlighting\nthe difficulty of comprehending and debugging AI-generated code,\nand Perry et al .finding that programmers who used AI tools were\nmore likely to write insecure code that included more system vul-\nnerabilities [ 25]. Other studies focused on how developers process\nthe output of AI tools. For example, Liao and Sundar discussed\nhow users make trust judgments of AI output, engaging in either\nsystematic processing (requiring a careful evaluation of the output)\nor heuristic processing (involving the use of heuristics to make\nquick, yet error-prone, trust assessments) [19].\nIn the computing education research space, two recent studies\nby Shihab et al .[35] and Shah et al .[33] studied upper-division\nCS students\u2019 interactions with AI tools while working on existing\ncodebases. Shihab et al .conducted a within-subjects experiment\nand found that students completed the tasks with Copilot 35%\nfaster, and that Copilot reduced the amount of time programmers\nspent writing code by 11 percentage points and the amount of time\nspent performing web searches reduced by 12 percentage points\n[35]. Similarly, Shah et al .analyzed the prompting strategies of 48\nstudents as they completed a task to add a feature to an open-source\ncodebase, showing that students preferred to interact with Copilot\nchat to comprehend or generate code and reporting a higher trustin Copilot\u2019s code comprehension features than its code generation\nfeatures [33].\nThe prior studies mentioned in this section describe workflows\nwith AI tools where programmers are still actively interacting with\nthe generated code. For example, both Barke et al ., Liang et al .,\nand Vaithilingam et al .highlight how programmers still spent time\nreviewing and understanding the code output, especially when\ntrying to debug. However, as discussed in Section 2.1, vibe coding\nplatforms introduce an even greater degree of abstraction between\nprogrammers and their code compared to earlier AI tools like Copi-\nlot and ChatGPT. This increased separation makes it especially\nimportant to study how the workflow affects intermediate pro-\ngrammers \u2014 such as our advanced software engineering students\n\u2014 who have enough experience to understand and occasionally\nreview code, but may not be proficient or confident in debugging\ncomplex implementations by hand.\n2.2.2 How Novice Programmers Use AI Tools. Plenty of work has\nanalyzed how students program with AI tools [ 1,5,21,28,29], how\nthese tools impact the help-seeking landscape [ 6,10,16,23], and\nhow to teach in the age of generative AI [ 7,16,17,26,37]. Given\nour study\u2019s focus on how novice and intermediate programmers ap-\nproach vibe coding, we will discuss prior works that have observed\nhow novices interact with various AI tools for programming.\nNovice programmers tend to exhibit ineffective prompting and\nverification strategies, with multiple studies showing that students\ntried to write prompts that would solve the entire problem at once\n[1,14,33]. Studies have also highlighted novices\u2019 ineffective verifi-\ncation strategies when working with AI generated code [ 14,28,29].\nKazemitabaar et al .showed that CS1 students only ran the AI gen-\nerated code 60% of the time, and 13% submitted AI-generated code\nwithout executing the code at all [ 14]. With an emphasis on non-\ntechnical end users, Zamfirescu-Pereira et al .discussed their strug-\ngles with AI prompting [ 41]. The authors found a lack of effective\nprompting strategies among end users, including limited examples\nor relevant details in their prompts [ 41]. The study also discussed\nusers\u2019 tendency to over-generalize the AI tool\u2019s behavior, especially\nwhen verifying the tool\u2019s handling of a particular input (e.g., lack\nof extensive testing before making a judgment) [ 41]. Though the\nparticipants in our study have more programming experience, the\nresults presented by Zamfirescu-Pereira et al .provide an example\nof how lay users prompt and verify AI systems they work with.\nStudies have aimed to explain why novices struggle when pro-\ngramming with AI. Lucchetti et al .attributed novices\u2019 ineffective\nprompting to a lack of effective vocabulary and relevant context for\nthe AI to generate code [ 21]. Prather et al .and Tankelevitch et al .\nargued that some of the challenges are metacognitive. For example,\nnovice students struggle to verify the correctness of output, lead-\ning them down an incorrect path that may only compound their\nexisting struggles [29].\nIn short, the studies above focused on how novices use and strug-\ngle with first-generation AI tools that involve more programmer-\ncode interactions than vibe coding. In response, our study takes the\nfirst steps in understanding how novice programmers prompt and\nverify the output of a vibe coding platform that, by design, lowers\nthe interactions between programmers and code.\n\n--- Page 4 ---\nConference 2025, 2025, Location Geng et al.\n3 Methods\n3.1 Research Questions\nThe research questions for our study are as follows:\nRQ1 How do students interact with AI tools when engaging\nin a vibe coding workflow?\nRQ2 How does prior programming experience influence the\nway students develop software in a vibe coding workflow?\n3.2 Study Context\n3.2.1 Recruitment. Our research study recruited students from\ntwo computer science classes at a large research-centric North\nAmerican institution, per our approved Human Subjects research\nprotocol. The first course is a CS1 course that primarily enrolls\nfirst- and second-year students who have some prior programming\nexperience before the course. The 10-week course is taught in the\nJava programming language and covers topics such as variables,\nconditionals, arrays, recursion, and basic object-oriented program-\nming. This CS1 class requires students to implement test cases and\npass hidden tests for programming assignments. The second course\nis an advanced Software Engineering (SWE) course that focuses on\nhow to comprehend, manage, and contribute to legacy code bases.\nStudents who are enrolled in this course are primarily students\nin their final year of their degree and have already completed the\ncore software engineering class in which they learn about AGILE\ndevelopment and create a mobile or web application from scratch\nin a team.\n3.2.2 Replit. We required students to use the Replit platform to\ncreate a web application. Replit, a browser-based, AI-integrated\nIDE, is designed specifically to support a vibe coding workflow\n[30]. It offers users the ability to \u201cPrompt your app ideas to life\n\u2014 no coding required\u201d \u2014 a quote taken directly from the Replit\nwebsite1. The Replit interface features an AI-powered chat panel,\nwhere users interact with Replit\u2019s chatbots, and a live preview of\nthe application\u2019s user interface. Initially, this preview is static, but\nit eventually becomes interactive and reflects current state of the\nunderlying application ( \u201cinterative prototype\u201d ). Notably, Replit does\nnot display code by default, unlike traditional IDEs such as VS\nCode or Eclipse. Replit centers the development experience around\nchatting with AI and interacting with the prototype, rather than\nsource code.\nWithin this interface, Replit offers two distinct chatbot tools,\n\u201cReplit Agent\u201d and \u201cReplit Assistant\u201d. The Agent is optimized for\nbuilding applications from the ground up; it can autonomously\nscaffold full-stack projects, configure environments, and directly\ncreate or modify files based on high-level prompts. In contrast,\nthe Assistant is tailored for working within an existing project. It\nsupports code-level refinement tasks such as debugging, explaining\ncode behavior, refactoring, and minor feature edits. It functions\nsimilarly to an in-line coding collaborator: users can ask questions\nor request specific changes, and the Assistant proposes contextual\nedits that must be explicitly approved before being applied.\n1https://replit.com/usecases/ai-app-builder3.3 Participants\nWe recruited 9 students from the CS1 course and 10 students from\nthe SWE course, as defined in Section 3.2.1.\nWithin the CS1 cohort (S11-S19), 4 students are pursuing com-\nputing or closely related majors and 5 are not. 6 identified as men\nand 3 as women. In terms of racial demographics, 7 identified as\nAsian or Asian American and 2 as White or Caucasian. Students\nself-declared their experience levels in programming, with 6 partic-\nipants reporting confidence in building a small class project and 3\nreporting only basic programming skills. All 9 participants reported\nhaving no software engineering internship experiences that involve\nworking with legacy code bases.\nUnlike our introductory cohort, all 10 students from the SWE\ncourse (S1-S10) have declared majors in computing. 8 identified as\nmen and 2 as women. The racial demographics consisted of 7 Asian\nor Asian Americans, 2 Chicanx or Latinx, and 1 White or Caucasian.\nStudents self-reported being capable of implementing projects of\nvarying levels, with 4 comfortable building small projects, 4 devel-\noping intermediate projects, and 2 working on large projects and\ncontributing to large codebases. Most of the participants (6 out of\n10) reported internship experience, and 9 out of 10 aspire to become\nprofessional software engineers.\n3.4 Study Procedure\nThe study sessions were conducted one-on-one on Zoom by the\nfirst author and a student. To refine task clarity and assess the\nstudy procedure, we first piloted it with 6 students from the same\ninstitution, where 2 students were from the same CS1 course, and\n4 students were third and fourth year undergraduate students with\na similar background as the SWE students. The finalized study\nprocedure takes 2 hours and breaks down as follows:\nPre-task training (30 minutes): The student first watched a 6-\nminute tutorial recorded by an author, which demonstrates using\nReplit Agent and Assistant, accessing the codebase, and interacting\nwith the Replit prototype. Next, the researcher introduced the think-\naloud method through a live demo. Students then practiced using\nReplit while thinking aloud with a sample task. This training phase\nwas not recorded.\nDevelopment session (60 minutes, timed): We designed an open-\nended task to develop a personal budget management web applica-\ntion using Replit in students\u2019 personal browsers. Specifically, in our\ntask description, we mentioned that the web application should be\nfunctioning and allow a user to do the following: 1) set a general\nmonthly budget; 2) break that budget down into categories (e.g.,\nfood, rent, entertainment); 3) input their actual expenses as the\nmonth progresses; and 4) track how their spending compares to\ntheir goals. The task description was intentionally open-ended to\nencourage a diverse range of student interpretations and resulting\napplication designs. Students were free to engage with the require-\nments as they saw fit and could make use of any features within the\nReplit environment, including the AI agent, assistant, code editor,\nconsole, and preview window. Additionally, students can use any\nexternal tools like ChatGPT as support.\nThe timer starts as a student opens and reads the task descrip-\ntion document. While completing the task, the student followed\n\n--- Page 5 ---\nExploring Student-AI Interactions in Vibe Coding Conference 2025, 2025, Location\nTable 1: Labeling scheme based on student-AI interactions, divided into four major categories.\nLabel Definition\nInteracting with Prototype\nRefresh Prototype Reloading the tab displaying the prototype\nTest Common Case Interacting with the prototype using typical inputs or actions that reflect normal usage;\nrepeated clicks or filling out a form count as a single case\nTest Edge Case Interacting with the prototype using unusual, invalid, or boundary inputs such as negative\nnumbers and empty fields\nWriting a Prompt\nDebug Prompting to address an error, bug, or system failure\nAdd/Remove/Update Core Feature Prompting to request any changes to a core feature (e.g. adding, checking, deleting, or editing\nbudgets, budget categories, or expenses) that does not have an error or bug\nAdd/Remove/Update Non-core Feature Prompting to request any changes to a non-core feature (e.g. UI elements, spending graphs)\nthat does not have an error or bug\nAsk Clarification Questions Prompting to ask the AI to explain, define, or elaborate on a concept, term, feature, or technical\ndetails\nBrainstorm Ideas Prompting for open-ended ideas or approaches without a clearly defined solution\nOther Prompting actions not covered by other labels, such as simulating a test case or responding\nto the AI\u2019s questions\nManaging Replit Workflow\nAccept Code Change Accepting a code modification proposed by the Replit Assistant using the built-in apply-\nsuggestion feature\nPause AI During Generation Interrupting the AI\u2019s response before completion by clicking a pause or stop button in the\nReplit interface\nResume AI Generation After Pause Resuming a paused AI response by clicking a continue or resume button in the Replit interface\nRevert to Checkpoint Reverting the code to a previous version by selecting a checkpoint created during an earlier\nAI response\nLoad Preview from Checkpoint Loading a preview of code from a previous checkpoint without reverting to it\nApprove Plan with X Additional Features Approving 0 or more optional features from the Replit Agent\u2019s initial project plan\nEngaging with Code/log\nInterpret Spending significant time analyzing code, console logs, or other development-related outputs.\nEdit Modifying code, console logs, or other development-related outputs\nthe think-aloud protocols to verbalize thoughts, decisions, and in-\ntentions. The recording captured the student\u2019s voice and screen\nactions, and optionally, the student could also turn on the camera.\nThroughout the session, the researcher observed student behaviors\nand took notes to capture key observations.\nPost-task interview (30 minutes): After the development session,\nthe student immediately participated in a recorded semi-structured\ninterview, where they recapped and reflected on their interactions\nwith AI tools, reasoning behind their actions, and emotional re-\nsponses during the session.\n3.5 Data Analysis\nAfter transcribing all recordings, two authors of this paper con-\nducted a qualitative thematic analysis to identify and label the\nactions students performed during the development session. The\ntwo authors first jointly open-coded one session from a CS1 student\nand one session from an SWE student. Then, the authors discussed\nto form an initial set of labels, and each author independently ap-\nplied these labels to the remaining 17 videos. The authors engaged\nin periodic discussions to refine the labeling scheme and resolve\ndiscrepancies, revisiting earlier videos as needed. Finally, the two\nauthors met synchronously to reach a negotiated agreement [ 9,32]on all applied labels, resolving any remaining differences and final-\nizing a consistent labeling scheme shown in Table 1.\nTo directly address our research questions, we labeled observable,\nscreen-recorded student actions when interacting with the Replit\nIDE and when optionally interacting with an external Generative\nAI tool, ChatGPT. In addition to the labeling scheme, we recorded\nthe prompts, code files, error messages following an interaction\nalong with other information that provides context to the labels as\n\u201cartifacts\u201d to provide further context. Although the focus of the study\nis on the student behaviors themselves, we also refer to students\u2019\nthink-aloud utterances, and responses during the semi-structured\ninterviews.\nIn addition to the labeling scheme (Table 1), we also recorded\nwhich AI tool the student used for Writing a Prompt (Replit Agent,\nReplit Assistant, ChatGPT). We also created sublabels to analyze\nprompts under the Writing a Prompt \u2014 Debug label (Table 2).\n4 Results\n4.1 RQ1: Student-AI Interactions in Vibe Coding\nOverview. The overall interactions with AI tools across the 19\nparticipants were shown in Figure 1. Across all students, the most\nprevalent label was Interacting with Prototype (63.61% of all labels, n\n\n--- Page 6 ---\nConference 2025, 2025, Location Geng et al.\nFigure 1: Distribution of labels across all 19 students, grouped by course. Each bar represents a single student, segmented by\ninteraction categories: Interacting with Prototype ,Writing a Prompt ,Engaging with Code/log , and Managing Replit Workflow .\nS1-S10 are SWE students and S11-S19 are CS1 students.\nTable 2: Writing a Prompt \u2014 Debug Sublabeling Scheme.\nSublabel Definition\nError Message Including a partial or full system error message\nwhen describing a bug or issue\nFailing Case Describing the input, condition, or steps that\nled to a failure or error, including what pre-\nceded the problem\nCode-focused Referencing specific files, code snippets, or im-\nplementation details believed to be related to\nthe issue\nOther Details Providing other relevant information not cap-\ntured above, such as triggering AI-generated\ndebugging prompts (e.g., \u201cTroubleshoot this is-\nsue\u201d or \u201c\u201cAsk Agent to explain code\u201d )\nLow Context Debugging prompts with none of the above\ncheckboxes selected, showing little or no new\ninformation or direction when initiating or con-\ntinuing a debugging effort\n= 1164), followed by Writing a Prompt (20.60%, n = 377), Managing\nReplit Workflow (8.42%, n = 154), and Engaging with Code/log (7.38%,\nn = 135). However, differences emerged across cohorts: students\nin CS1 showed markedly higher proportions of Writing a Prompt\nlabels compared to SWE students, whereas Engaging with Code/log\ninteractions were more common among SWE students.\nRestarters. While prompting was a common behavior across\nparticipants, a subset of students (4 out of 19) exhibited a more\ndramatic interaction pattern: they restarted the entire project using\nthe Replit Agent mid-task. We term these students restarters (S1,\nS2, S14, S17). 3 out of the 4 restarters restarted primarily to simplify\ntheir interaction with the AI, citing overwhelming or ambiguous\nbehavior from earlier prompts. For instance, one student reflected\nthat they \u201casked the Replit to do way too many things, \u201d making it\ndifficult to identify specific issues. Another opted to \u201cbreak it downone task at a time\u201d after experiencing repeated failures. These be-\nhaviors suggest that some students use restart strategies not out of\nfailure alone, but as a form of iterative refinement and task decom-\nposition. The fourth restarter (S2) retained their original prompt\nbut added a sentence to explicitly request a Flask framework over\nReact due to familiarity and perceived simplicity, and they consid-\nered their restart as a means of architectural realignment rather\nthan functional simplification. In all cases, the restart decision re-\nflected metacognitive awareness about the limitations of debugging\nthrough prompting: students had attempted to fix issues in the\noriginal prototype but found the bugs unresolvable via continued\nAI interaction, prompting a fresh start instead.\nTable 3: Frequency of labels for Interacting with Prototype\nactions.\nLabel Count Percent\nTest Common Case 1067 91.67%\nRefresh Prototype 71 6.10%\nTest Edge Case 26 2.23%\nTotal 1164 100%\nInteracting with Prototype. First, we examine how students\ninteract with the prototypes generated by the AI tools (Table 3).\nThe overwhelming majority of these interactions (91.67%) involved\ntesting common use cases, while only 6.10% involved refreshing the\nprototype and a mere 2.23% involved edge case testing. Transcript\ndata suggests that prototype refreshes were typically prompted by\ntechnical limitations (e.g., Replit failing to maintain state across\npages) rather than as part of a deliberate debugging strategy. No-\ntably, no student wrote or executed unit tests during the study,\nindicating that their approach to testing was exclusively centered\non feature-level behaviors visible through the UI. The absence of\nstructured test practices, combined with the minimal edge case cov-\nerage, suggests that many students were unable to progress beyond\nbasic functionality, frequently encountering bugs that prevented\n\n--- Page 7 ---\nExploring Student-AI Interactions in Vibe Coding Conference 2025, 2025, Location\nmore in-depth testing. These patterns highlight the inherently iter-\native and sometimes unstable nature of vibe coding, where students\noften remain occupied with basic interactions and repeated trou-\nbleshooting, rather than advancing toward comprehensive feature\nvalidation.\nTable 4: Frequency of labels for Writing a Prompt actions.\nLabel Count Percent\nDebug 230 61.01%\nAdd/Remove/Update Non-core Feature 63 16.71%\nAdd/Remove/Update Core Feature 53 14.06%\nOther 16 4.24%\nAsk Clarification Question 12 3.18%\nBrainstorm Ideas 3 0.80%\nTotal 377 100%\nWriting a Prompt. To better understand what students\u2019 goals\nare when they are prompting, we analyzed the labels of all prompt-\ning behaviors for all 19 students (Table 4). The majority of prompts\n(61.01%) were used for debugging AI-generated code, followed\nby modifications to non-core features (16.71%) and core features\n(14.06%). Prompts related to brainstorming, clarification questions,\nor miscellaneous tasks were relatively rare (<5% each). These data\nindicate that students primarily engaged with AI tools to trou-\nbleshoot and refine partial implementations, rather than to build\nfunctionality from scratch. As one student described their strategy,\n\u201c...finding [bugs] myself, realizing what the problem was, and then\nputting it back into the Agent to solve it in like 2 seconds... \u201d reflects\nhow students often used AI to efficiently resolve implementation\nissues they had already identified.\nTable 5: Frequency of student prompts with different AI tools.\nAI Tool Count Percent\nReplit Assistant 271 50.94%\nReplit Agent 246 46.24%\nChatGPT 15 2.82%\nTotal 532 100%\nIn terms of which AI tools students used for prompting (Table 5),\nthe Replit Assistant accounted for a slight majority of prompting\ninteractions (50.94%), closely followed by Replit Agent (46.24%).\nChatGPT was used only in 2.82% of the prompt instances, likely\nreflecting its auxiliary role in the workflow. These numbers suggest\nthat while multiple AI tools were available, most of the prompting\noccurred within the embedded Replit interfaces, suggesting that\naccessibility and immediacy of tooling may play a significant role\nin shaping student behavior. As one student reflected, \u201cI didn\u2019t\nreally understand the real difference between Agent and Assistant...\nit kind of felt like they were the same thing\u201d , emphasizing how\nconceptual ambiguity may have contributed to balanced usage.\nAnother student explained, \u201cI\u2019ll kind of use ChatGPT and Copilot\nfor assistance on making small fixes, \u201d illustrating ChatGPT\u2019s more\noccasional and supporting role relative to the Replit-native tools.Table 6: Frequency of labels for Managing Replit Workflow\nactions.\nLabel Count Percent\nAccept Code Change 105 68.18%\nApprove Plan with X Additional Features 23 14.94%\nPause AI During Generation 12 7.79%\nRevert to Checkpoint 7 4.55%\nLoad Preview from Checkpoint 5 3.25%\nResume AI Generation After Pause 2 1.30%\nTotal 154 100%\nManaging Replit Workflow. In addition to prompting actions,\nstudents interacted with Replit in several other ways that reflect the\nvibe coding workflow with the Replit interface (Table 6). The most\ncommon action in this category was accepting Replit-proposed\ncode changes (68.18%), a manual decision-making step mandatory\nfor students working with Replit Assistant. After writing an initial\nprompt, a student had to approve an AI-generated implementation\nplan with optional additional features recommended by the Replit\nAgent (14.94%) based on the content of their first prompt. Some\nstudents also paused (7.79%) and resumed (1.30%) the AI gener-\nation process, indicating moments of review or reconsideration\nduring code generation. In addition, a few students used Replit\u2019s\nversion control features to either Revert to Checkpoint (4.55%) or\nLoad Preview from Checkpoint (3.25%), actions functionally similar\nto Git\u2019s revert andcheckout , respectively. These behaviors sug-\ngest that students occasionally found it necessary to backtrack after\na prompt yielded undesirable or destabilizing changes, emphasizing\nthe trial-and-error nature of vibe coding.\nEngaging with Code/log. Among the rare cases where students\nengaged with code and logs generated by Replit, an overwhelming\n90.37% of the interactions were reading and interpreting code (n\n= 122), and the remaining actions were direct edits (9.63%, n = 13).\nThis strong preference for interpretation over modification suggests\nthat students were often hesitant to alter AI-generated code, likely\ndue to limited familiarity with the underlying implementation. The\nvibe coding workflow may exacerbate this hesitation by distancing\nstudents from the logic and structure of AI-generated codebases.\nAs one student explained, \u201cBecause so much of it was just done by\nthe LLM, I had a lesser understanding of the codebase \u2014 rather than\nwhat I would do on my own, where I know what each line does. \u201d We\nfurther expand and compare Engaging with Code/log behaviors in\nSection 4.2.\n4.2 RQ2: Relations between Programming\nExperience and Vibe Coding Strategies\nEngaging with Code/log. To investigate how programming experi-\nence influences students\u2019 engagement with code, we compared the\nproportion of Engaging with Code/log actions (either interpreting or\neditting AI-generated code) between students in CS1 and SWE. As\nshown in Figure 2, SWE students exhibited a higher overall propor-\ntion of Engaging with Code/log behaviors (M = 0.35%, SD = 0.33%)\ncompared to CS1 students (M = 0.09%, SD = 0.19%), although the\ndifference approaches but does not reach statistical significance (p =\n.0548). Among individual behaviors, 9 of 10 SWE students engaged\n\n--- Page 8 ---\nConference 2025, 2025, Location Geng et al.\nFigure 2: Proportion of Engaging with Code/log labels per\nstudent, normalized by each student\u2019s total behavior labels.\nin code interpretation, and 3 of them also edited AI-generated code.\nBy contrast, 4 of 9 CS1 students interpreted code, and only 2 CS1\nstudents (S16 and S19) edited code. Notably, these two students\nwere the only ones in their cohort who self-identified as capable of\ndeveloping intermediate-level programs and debugging, suggesting\nthat students with greater programming experience are more likely\nto engage directly with the structure and content of AI-generated\ncode.\nWriting a Prompt. Students from CS1 and SWE also showed\nnoticeably different behaviors in two aspects when articulating\ntheir prompts during vibe coding, characterized as Low Context and\nCode-focused (defined in Table 2). Low Context prompts are often\nvague and offer limited actionable context, such as \u201cThe buttons\nappear but they don\u2019t work at all, \u201d or follow-up phrases like \u201cit\u2019s\nstill not working. \u201d Figure 3 compares the proportion of low context\nprompts across the two groups and reveals that CS1 students had a\nsignificantly higher proportion of such prompts (M = 28.89%, SD =\n20.66%) than SWE students (M = 8.39%, SD = 6.99%, p < .01).\nIn contrast, Code-focused prompts that reference specific seg-\nments of AI-generated code or logs were more prevalent among\nSWE students. For example, one student wrote, \u201cIn line 118 of\nroutes.ts, are you passing the correct data into storage.createStorage?\u201d .\nAnother student wrote, \u201cIn the <Header> component, error occurred\n\u2019cannot read properties of undefined (reading \u2019month\u2019)\u2019, set the month\nto be the current month at the time the user is using the web app. \u201d\nThese kinds of prompt demonstrate precise understanding and rea-\nsoning about the program\u2019s structure and behavior. 7 out of 10\nSWE students used code-focused prompts, while only 1 student\nfrom the CS1 group (S16) did so. This difference suggests that stu-\ndents with greater programming experience were more capable\nof constructing prompts that referenced concrete implementation\ndetails, allowing them to more effectively communicate intent and\ntroubleshoot with the AI system.\nInteracting with Prototype. We also analyzed the extent to\nwhich students conducted edge case testing while Interacting with\nPrototype using uncommon or boundary inputs. 6 of 10 SWE stu-\ndents performed at least one edge case test, compared to 4 of 9 in\nCS1. While this difference is modest, it may reflect task progression:\nFigure 3: Proportion of Low Context prompts per student,\nnormalized by each student\u2019s total number of prompts.\nstudents who encountered repeated errors in basic functionality\noften focused on common case testing and did not reach a point\nwhere edge cases were necessary. Since edge case testing is intro-\nduced early in many computing curricula, including this CS1 course,\nthe observed difference is less likely due to instruction and more\nlikely tied to whether the student\u2019s implementation had stabilized\nenough to allow for more nuanced evaluation.\n5 Discussion\nOur study reveals how vibe coding \u2014 delegating coding, debugging,\nand design to generative AI via high-level prompts \u2014 reshapes stu-\ndent software development workflows. Below, we interpret key pat-\nterns, address implications for education/AI design, and acknowl-\nedge limitations.\n5.1 Implications for Educators\nWith the recent arrival of agentic AI and vibe coding, there is\nconsiderable discourse around vibe coding as a viable practice for\ncreating software. The fact that minimal supervision is part of\nvibe coding may suggest that someone completely unfamiliar with\nprogramming could just dive right into the practice. Our findings\nraise caution here, in that students from both the introductory and\nthe advanced programming classes used the same set of skills when\ninteracting with AI: prompting to debug, performing feature-level\ntests, and engaging with code. Although students interacted with\ncode infrequently, the advanced programming students were still\nwilling to do so in a way a complete novice (or even introductory\nprogramming students) might be unable to do.\nThe many examples of professional software engineers vibe cod-\ning, apparently with ease [ 32], may be misleading to non-experts as\nexperts can judge whether and when to intervene. Even for experts,\nthe conversation remains around creating \u201cweekend projects\u201d and\nnot large software products that require maintenance. For students,\nwho lack this safety net of expertise, vibe coding can obscure code\ncomprehension, encourage reliance on feature-level testing over\nmore structured validation methods, and hinder long-term skill de-\nvelopment if not scaffolded appropriately. The utility of vibe coding\nas a pedagogical tool or professional practice thus remains an open\n\n--- Page 9 ---\nExploring Student-AI Interactions in Vibe Coding Conference 2025, 2025, Location\nquestion for the community. Our work offers early insight into how\nprogramming students (both introductory and advanced) navigate\nthis space and where support may be most needed.\nInteracting with the prototype was the most common behavior\nfor both introductory and advanced students. Both groups exper-\nimented with the prototype to identify what worked and what\ndidn\u2019t, then used that information to ask Replit to correct short-\ncomings. The fact that students were so quick to move to testing is\nan encouraging finding for teaching testing in computing courses.\nIt\u2019s unclear why students were so eager to test, but it can poten-\ntially be attributed to 1) the interface making the prototype more\naccessible than code, and 2) students are equipped with enough\ntechnical intuition and interest to engage with a website. While\nboth experts and non-experts performed frequent feature-based\ntesting, experts are far more apt to perform edge-case tests. Teach-\ning proper testing techniques with common cases and edge cases\nremains an important topic in computing education. Furthermore,\nsince Replit encouraged feature-based testing over unit tests, the\nfuture of teaching unit-test-driven development in the age of AI\nremains uncertain.\nOur findings demonstrate that introductory and advanced pro-\ngramming students have different prompting styles. The advanced\nstudents are far more apt to communicate with Replit using prompts\nthat contain computational ideas and details than introductory stu-\ndents. These sophisticated prompts suggest, perhaps unsurprisingly,\nthat students will need to be taught these computational ideas to\ncommunicate with Replit effectively. In particular, prompting in\nvibe coding is not simply about phrasing requests but about con-\nveying intent grounded in code structure, logic, and context. This\nmeans instruction should go beyond syntax and include how to\nanalyze generated output, translate errors into specific follow-up\nquestions, and iteratively refine vague or failed prompts. Indeed,\nthis finding supports the work of Lucchetti et al. who found that\nprogramming novices often author prompts that lack important\ndetails for the AI [21]. Overall, our work provides further evidence\nthat students will need to learn computational skills and prompting\nskills to successfully interact with these models.\n5.2 Limitations\nWhile our study offers timely insights into how students interact\nwith AI tools in a vibe coding workflow, several limitations must\nbe acknowledged.\nSample representation. Our study was conducted at a single\nNorth American institution with students recruited from two com-\nputing courses, and participation was voluntary. This recruitment\nstrategy may introduce self-selection bias, as students who opted\ninto the study may be more confident, motivated, or interested in\nAI tools than their peers. Consequently, the behaviors observed\nmay reflect a more engaged or higher-performing subset of the stu-\ndent population. Expanding future studies to multiple institutions\nand course settings would strengthen the applicability of findings\nacross contexts.\nPlatform specificity. Our findings are grounded entirely in\nstudent experiences with the Replit platform, which is designed\naround an AI chat interface and an interactive prototype that de-\nprioritizes code visibility. While this design makes Replit ideal forexamining vibe coding, it does not reflect the full spectrum of vibe\ncoding platforms. Other tools (e.g., Cursor, Copilot Agents) may pro-\nvide greater transparency into generated code, tighter integration\nwith traditional IDE workflows, or different prompting paradigms\naltogether. As such, our results should not be generalized to allvibe\ncoding environments without caution. Future work could incorpo-\nrate multiple platforms to assess how specific design features shape\nstudent behavior.\nFocus on observed behaviors, not underlying reasoning.\nOur analysis centers on students\u2019 observable actions (e.g., prompt-\ning, testing, engaging with code) and classifies them using a struc-\ntured labeling scheme. However, we did not systematically analyze\nstudents\u2019 spoken reasoning or reflective interview data to explain\nwhy those behaviors occurred. While the think-aloud protocol and\npost-task interviews provide valuable context, we primarily used\nthem to validate labeled behaviors rather than drive deeper the-\nmatic or cognitive analysis. As a result, we cannot fully characterize\nstudents\u2019 intentions, mental models, metacognitive strategies, or\nemotional responses \u2014 essential elements for understanding the\nlearning and decision-making processes behind their interactions\nwith AI tools. A complementary analysis of verbal data could pro-\nvide richer insight into students\u2019 goals, frustrations, and strategies\nwhile vibe coding.\nInteraction frequency is not equivalent to time on task. All\nquantitative results in this paper are based on counts of interaction\nlabels (e.g., number of prompts, code edits, test cases). However, raw\nfrequency does not capture how long students spent on different\nactivities. For instance, a single instance of code interpretation may\ninvolve several minutes of analysis, while five quick prompts could\nspan less than a minute in total. As such, interpreting our frequency\ndata as proportional effort or cognitive investment would be mis-\nleading. Future work should incorporate duration-based metrics or\ntemporal coding to more accurately represent time allocation and\nsustained engagement with specific activities.\n6 Conclusion\nThis study offers first insights into how computing students interact\nwith a vibe-coding tool (Replit) while creating a functional and use-\nful web app. Through qualitatively analyzing observations of how\nstudents vibe code, we uncovered students\u2019 interaction patterns\nwith Replit and the differences between students in introductory\nand advanced computer science courses. Our findings include 1)\nthe bulk of student interactions with Replit are testing and debug-\nging, and 2) advanced students are more capable of prompting with\ncomputational sophistication and are more likely to interact with\nthe codebase. Our findings offer insights to educators on the skills\nthat students use when vibe coding while buttressing findings in\nGenAI broadly \u2014 that testing and debugging remain critical in the\nAI era [ 26,27] and that students need training in prompting [ 21].\n7 Acknowledgments\nThis work is supported by the National Science Foundation Gradu-\nate Research Fellowship Program and NSF Award #2417374, and by\nthe Google Award for Inclusion Research Program. Any opinions,\nfindings, and conclusions or recommendations expressed in this\nmaterial are those of the authors(s) and do not necessarily reflect\nthe views of the National Science Foundation or Google.\n\n--- Page 10 ---\nConference 2025, 2025, Location Geng et al.\nReferences\n[1]Matin Amoozadeh, Daye Nam, Daniel Prol, Ali Alfageeh, James Prather, Michael\nHilton, Sruti Srinivasa Ragavan, and Amin Alipour. 2024. Student-AI Interaction:\nA Case Study of CS1 students. In Proceedings of the 24th Koli Calling International\nConference on Computing Education Research . Article 13. https://doi.org/10.1145/\n3699538.3699567\n[2]Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. Proc. ACM\nProgram. Lang. 7, OOPSLA1 (2023), 27 pages. https://doi.org/10.1145/3586030\n[3]Xiang Chen, Chaoyang Gao, Chunyang Chen, Guangbei Zhang, and Yong Liu.\n2025. An Empirical Study on Challenges for LLM Application Developers. ACM\nTrans. Softw. Eng. Methodol. (2025). https://doi.org/10.1145/3715007\n[4]Cursor. 2025. Cursor - The AI Code Editor. https://cursor.com/en. Accessed:\n2025-07-25.\n[5]Paul Denny, Viraj Kumar, and Nasser Giacaman. 2023. Conversing with copilot:\nExploring prompt engineering for solving CS1 problems using natural language.\nInProceedings of the 54th ACM technical symposium on computer science education\nV. 1. 1136\u20131142.\n[6]Paul Denny, Stephen MacNeil, Jaromir Savelka, Leo Porter, and Andrew Luxton-\nReilly. 2024. Desirable characteristics for AI teaching assistants in programming\neducation. In Proceedings of the 2024 conference on Innovation and Technology in\nComputer Science Education V. 1 . 408\u2013414.\n[7]Paul Denny, James Prather, Brett A Becker, James Finnie-Ansley, Arto Hellas,\nJuho Leinonen, Andrew Luxton-Reilly, Brent N Reeves, Eddie Antonio Santos,\nand Sami Sarsa. 2024. Computing education in the era of generative AI. Commun.\nACM 67, 2 (2024), 56\u201367.\n[8]Ya Gao. 2024. Research: Quantifying GitHub Copilot\u2019s impact in the enter-\nprise with Accenture. https://github.blog/news-insights/research/research-\nquantifying-github-copilots-impact-in-the-enterprise-with-accenture/\n[9]D.R. Garrison, M. Cleveland-Innes, Marguerite Koole, and James Kappelman.\n2006. Revisiting methodological issues in transcript analysis: Negotiated coding\nand reliability. The Internet and Higher Education 9, 1 (2006), 1\u20138. https://doi.\norg/10.1016/j.iheduc.2005.11.001\n[10] Irene Hou, Owen Man, Kate Hamilton, Srishty Muthusekaran, Jeffin Johnykutty,\nLeili Zadeh, and Stephen MacNeil. 2025. \u2019All Roads Lead to ChatGPT\u2019: How\nGenerative AI is Eroding Social Interactions and Student Learning Communi-\nties. In Proceedings of the 30th ACM Conference on Innovation and Technology in\nComputer Science Education V. 1 . 79\u201385. https://doi.org/10.1145/3724363.3729024\n[11] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. 2025.\nFrom LLMs to LLM-based Agents for Software Engineering: A Survey of Current,\nChallenges and Future. arXiv:2408.02479 [cs.SE] https://arxiv.org/abs/2408.02479\n[12] Andrej Karpathy. 2025. https://x.com/karpathy/status/1886192184808149383\n[13] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J Ericson, David\nWeintrop, and Tovi Grossman. 2023. Studying the effect of AI code generators\non supporting novice learners in introductory programming. In Proceedings of\nthe 2023 CHI conference on human factors in computing systems . 1\u201323.\n[14] Majeed Kazemitabaar, Xinying Hou, Austin Henley, Barbara Jane Ericson, David\nWeintrop, and Tovi Grossman. 2024. How Novices Use LLM-based Code Gen-\nerators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment. In\nProceedings of the 23rd Koli Calling International Conference on Computing Educa-\ntion Research . Article 3, 12 pages. https://doi.org/10.1145/3631802.3631806\n[15] Majeed Kazemitabaar, Oliver Huang, Sangho Suh, Austin Henley, and Tovi Gross-\nman. 2025. Exploring the design space of cognitive engagement techniques with\nAI-generated code for enhanced learning. In Proceedings of the 30th International\nConference on Intelligent User Interfaces . 695\u2013714.\n[16] Majeed Kazemitabaar, Runlong Ye, Xiaoning Wang, Austin Henley, Paul Denny,\nMichelle Craig, and Tovi Grossman. 2024. CodeAid: Evaluating a classroom\ndeployment of an LLM-based programming assistant that balances student and\neducator needs. In Proceedings of the 2024 chi conference on human factors in\ncomputing systems . 1\u201320.\n[17] Sam Lau and Philip Guo. 2023. From\" Ban it till we understand it\" to\" Resistance is\nfutile\": How university programming instructors plan to adapt as more students\nuse AI code generation and explanation tools such as ChatGPT and GitHub\nCopilot. In Proceedings of the 2023 ACM Conference on International Computing\nEducation Research-Volume 1 . 106\u2013121.\n[18] Jenny T. Liang, Chenyang Yang, and Brad A. Myers. 2024. A Large-Scale Survey\non the Usability of AI Programming Assistants: Successes and Challenges. In\nProceedings of the IEEE/ACM 46th International Conference on Software Engineering .\nArticle 52, 13 pages. https://doi.org/10.1145/3597503.3608128\n[19] Q.Vera Liao and S. Shyam Sundar. 2022. Designing for Responsible Trust in\nAI Systems: A Communication Perspective. In Proceedings of the 2022 ACM\nConference on Fairness, Accountability, and Transparency . 1257\u20131268. https:\n//doi.org/10.1145/3531146.3533182\n[20] Mark Liffiton, Brad E Sheese, Jaromir Savelka, and Paul Denny. 2023. CodeHelp:\nUsing large language models with guardrails for scalable support in program-\nming classes. In Proceedings of the 23rd Koli Calling International Conference onComputing Education Research . 1\u201311.\n[21] Francesca Lucchetti, Zixuan Wu, Arjun Guha, Molly Q Feldman, and Carolyn Jane\nAnderson. 2024. Substance Beats Style: Why Beginning Students Fail to Code\nwith LLMs. arXiv preprint arXiv:2410.19792 (2024).\n[22] Microsoft Copilot. 2025. Copilot and AI agents. https://www.microsoft.com/en-\nus/microsoft-copilot/copilot-101/copilot-ai-agents. Accessed: 2025-07-25.\n[23] Ismael Villegas Molina, Audria Montalvo, Benjamin Ochoa, Paul Denny, and Leo\nPorter. 2024. Leveraging LLM tutoring systems for non-native english speakers\nin introductory CS courses. arXiv preprint arXiv:2411.02725 (2024).\n[24] Ipek Ozkaya. 2023. Application of Large Language Models to Software Engineer-\ning Tasks: Opportunities, Risks, and Implications. IEEE Software 40, 3 (2023), 4\u20138.\nhttps://doi.org/10.1109/MS.2023.3248401\n[25] Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. 2023. Do Users\nWrite More Insecure Code with AI Assistants?. In Proceedings of the 2023 ACM\nSIGSAC Conference on Computer and Communications Security . 2785\u20132799. https:\n//doi.org/10.1145/3576915.3623157\n[26] Leo Porter and Daniel Zingaro. 2024. Learn AI-assisted Python programming: with\nGitHub Copilot and ChatGPT . Simon and Schuster.\n[27] James Prather, Juho Leinonen, Natalie Kiesler, Jamie Gorson Benario, Sam Lau,\nStephen MacNeil, Narges Norouzi, Simone Opel, Vee Pettit, Leo Porter, et al .2025.\nBeyond the hype: A comprehensive review of current trends in generative AI\nresearch, teaching practices, and tools. 2024 Working Group Reports on Innovation\nand Technology in Computer Science Education (2025), 300\u2013338.\n[28] James Prather, Brent N. Reeves, Paul Denny, Brett A. Becker, Juho Leinonen,\nAndrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, and Eddie Antonio\nSantos. 2023. \u201cIt\u2019s Weird That it Knows What I Want\u201d: Usability and Interactions\nwith Copilot for Novice Programmers. ACM Trans. Comput.-Hum. Interact. 31, 1,\nArticle 4 (2023), 31 pages. https://doi.org/10.1145/3617367\n[29] James Prather, Brent N Reeves, Juho Leinonen, Stephen MacNeil, Arisoa S Ran-\ndrianasolo, Brett A. Becker, Bailey Kimmel, Jared Wright, and Ben Briggs. 2024.\nThe Widening Gap: The Benefits and Harms of Generative AI for Novice Pro-\ngrammers. In Proceedings of the 2024 ACM Conference on International Computing\nEducation Research - Volume 1 . 469\u2013486. https://doi.org/10.1145/3632620.3671116\n[30] replit. 2025. replit - Build apps and sites with AI. https://replit.com/. Accessed:\n2025-07-25.\n[31] Ranjan Sapkota, Konstantinos I. Roumeliotis, and Manoj Karkee. 2025. Vibe\nCoding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic\nAI. (2025). https://arxiv.org/abs/2505.19443\n[32] Advait Sarkar and Ian Drosos. 2025. Vibe coding: programming through conver-\nsation with artificial intelligence. (2025). https://arxiv.org/abs/2506.23253\n[33] Anshul Shah, Anya Chernova, Elena Tomson, Leo Porter, William G. Griswold,\nand Adalbert Gerald Soosai Raj. 2025. Students\u2019 Use of GitHub Copilot for Work-\ning with Large Code Bases. In Proceedings of the 56th ACM Technical Symposium\non Computer Science Education V. 1 . 1050\u20131056. https://doi.org/10.1145/3641554.\n3701800\n[34] Inbal Shani. 2023. Survey reveals AI\u2019s impact on the developer expe-\nrience . https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-\ndeveloper-experience/\n[35] Md Istiak Hossain Shihab, Christopher Hundhausen, Ahsun Tariq, Summit Haque,\nYunhan Qiao, and Brian Mulanda. 2025. The Effects of GitHub Copilot on Comput-\ning Students\u2019 Programming Effectiveness, Efficiency, and Processes in Brownfield\nProgramming Tasks. arXiv preprint arXiv:2506.10051 (2025).\n[36] Lev Tankelevitch, Viktor Kewenig, Auste Simkute, Ava Elizabeth Scott, Advait\nSarkar, Abigail Sellen, and Sean Rintel. 2024. The metacognitive demands and\nopportunities of generative AI. In Proceedings of the 2024 CHI Conference on\nHuman Factors in Computing Systems . 1\u201324.\n[37] Annapurna Vadaparty, Daniel Zingaro, David H Smith IV, Mounika Padala, Chris-\ntine Alvarado, Jamie Gorson Benario, and Leo Porter. 2024. CS1-LLM: Integrating\nLLMs into CS1 instruction. In Proceedings of the 2024 conference on Innovation\nand Technology in Computer Science Education v. 1 . 297\u2013303.\n[38] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered by\nLarge Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems . Article 332, 7 pages. https://doi.org/10.\n1145/3491101.3519665\n[39] Ruotong Wang, Ruijia Cheng, Denae Ford, and Thomas Zimmermann. 2024.\nInvestigating and Designing for Trust in AI-powered Code Generation Tools. In\nProceedings of the 2024 ACM Conference on Fairness, Accountability, and Trans-\nparency . 1475\u20131493. https://doi.org/10.1145/3630106.3658984\n[40] Bingyang Wei. 2024. Requirements are All You Need: From Requirements to Code\nwith LLMs. In 2024 IEEE 32nd International Requirements Engineering Conference\n(RE). 416\u2013422. https://doi.org/10.1109/RE59067.2024.00049\n[41] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny Can\u2019t Prompt: How Non-AI Experts Try (and Fail) to Design\nLLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Com-\nputing Systems . Article 437, 21 pages. https://doi.org/10.1145/3544548.3581388",
  "project_dir": "artifacts/projects/enhanced_cs.HC_2507.22614v1_Exploring_Student_AI_Interactions_in_Vibe_Coding",
  "communication_dir": "artifacts/projects/enhanced_cs.HC_2507.22614v1_Exploring_Student_AI_Interactions_in_Vibe_Coding/.agent_comm",
  "assigned_at": "2025-08-03T20:52:49.403736",
  "status": "assigned"
}